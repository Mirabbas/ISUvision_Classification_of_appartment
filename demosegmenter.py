# -*- coding: utf-8 -*-
"""DemoSegmenter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/CSAILVision/semantic-segmentation-pytorch/blob/master/notebooks/DemoSegmenter.ipynb

# Semantic Segmentation Demo

This is a notebook for running the benchmark semantic segmentation network from the the [ADE20K MIT Scene Parsing Benchchmark](http://sceneparsing.csail.mit.edu/).

The code for this notebook is available here
https://github.com/CSAILVision/semantic-segmentation-pytorch/tree/master/notebooks

It can be run on Colab at this URL https://colab.research.google.com/github/CSAILVision/semantic-segmentation-pytorch/blob/master/notebooks/DemoSegmenter.ipynb
"""



"""### Environment Setup

First, download the code and pretrained models if we are on colab.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# # Colab-specific setup
# !(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit 
# pip install yacs 2>&1 >> install.log
# git init 2>&1 >> install.log
# git remote add origin https://github.com/CSAILVision/semantic-segmentation-pytorch.git 2>> install.log
# git pull origin master 2>&1 >> install.log
# DOWNLOAD_ONLY=1 ./demo_test.sh 2>> install.log


from selenium import webdriver
import re
from selenium.common.exceptions import NoSuchElementException, WebDriverException
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from pyvirtualdisplay import Display
import json
import time
import vk_api
class Yandex(object):
  def __init__(self):
    self.vk = vk_api.VkApi(
     token="d71b167752a82ff5669ff1cc8f0503c2")
    self.chrome_options = Options()
    self.chrome_options.add_argument("headless")
    self.chrome_options.add_argument('window-size=1920x935')
    self.driver = webdriver.Chrome(executable_path="C:/Users/pitonhik/Downloads/chromedriver.exe")
    self.tabs = self.driver.window_handles
  def get_url(self,path):
    if path[:4] == 'http':
        return path
    else:
        return vk_api.VkUpload(self.vk).photo_messages(photos=path)[0]['sizes'][4]['url']




  def make_url_yandex(self,url):
    s = url.replace('/','%2F')
    url = s.replace(':','%3A')
    
    rez = 'https://yandex.ru/images/search?url=' + url + '&rpt=imageview&from='
    return rez
  def get_info_yangex(self,url):
    url = self.get_url(url)
    
    u = self.make_url_yandex(url)
    
    self.driver.get(u)
               
    time.sleep(1)
    a = self.driver.find_elements_by_tag_name('a')
    text = []
    for i  in range(len(a)):
     try:
      at = a[i].get_attribute("tone")
      if at=='gray':
         text.append(a[i].text)
     except:
        True
    text = text[::-1]
    text = text[:4]
    try:
     g = self.driver.find_elements_by_class_name('MarketProduct-Inner')
    
     
    
     s1 = g[0].find_elements_by_tag_name('span')
     s2=g[1].find_elements_by_tag_name('span')
    
     price = int(s1[1].text) + int(s2[1].text)
     price = int(price/2)
    except:
        price = False
    return text[0] , price
  def close(self):
      self.driver.close()


# System libs
import os, csv, torch, numpy, scipy.io, PIL.Image, torchvision.transforms
# Our libs
from PIL import ImageDraw
from mit_semseg.models import ModelBuilder, SegmentationModule
from mit_semseg.utils import colorEncode

colors = scipy.io.loadmat('data/color150.mat')['colors']
names = {}
with open('data/object150_info.csv') as f:
    reader = csv.reader(f)
    next(reader)
    for row in reader:
        names[int(row[0])] = row[5].split(";")[0]

def photo_save(name,kord,frame):
    img = frame.crop((kord[0],kord[1],kord[2],kord[3]))
    img.save(str(name)+'.jpg')
def visualize_result(img, pred, index=None):
   
    # filter prediction class if requested
    if index is not None:
     pred = pred.copy()
     pred[pred != index] = -1
     print(f'{names[index+1]}:')
     x1 = []
     x2 = []
     y1=[]
     y2=[]
     flag1 = True
     flag2= True
   
     
     for i in range(len(pred)):
        if (flag1 and (max(pred[i])>-1)):
            y1.append(i)
            flag1=False
        elif (flag2 and max(pred[len(pred)-1-i])>-1):
            
            ap = len(pred)-1-i
            
            y2.append(ap)
            flag2=False
     
     xpred = numpy.transpose(pred)
     flag1 = True
     flag2= True
     for i in range(len(xpred)):
        if flag1 and (max(xpred[i])>-1):
            x1.append(i)
            flag1=False
        elif flag2 and max(xpred[len(xpred)-1-i])>-1:
            
            ap = len(xpred)-1-i
          
            x2.append(ap)
            flag2=False
   
     pred_color = colorEncode(pred, colors).astype(numpy.uint8)
     
     x1 = min(x1)
     x2 = max(x2)
     y1 = min(y1)
     y2 = min(y2)

     # aggregate images and save
     im_vis = numpy.concatenate((img, pred_color), axis=1)
    
     
     #display(PIL.Image.fromarray(im_vis))
    
     return [x1,y1,x2,y2]
"""## Loading the segmentation model

Here we load a pretrained segmentation model.  Like any pytorch model, we can call it like a function, or examine the parameters in all the layers.

After loading, we put it on the GPU.  And since we are doing inference, not training, we put the model in eval mode.

# Новый раздел
"""

# Network Builders
net_encoder = ModelBuilder.build_encoder(
    arch='resnet50dilated',
    fc_dim=2048,
    weights='ckt/encoder_epoch_20.pth')
net_decoder = ModelBuilder.build_decoder(
    arch='ppm_deepsup',
    fc_dim=2048,
    num_class=150,
    weights='ckt/decoder_epoch_20.pth',
    use_softmax=True)

crit = torch.nn.NLLLoss(ignore_index=-1)
segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)
segmentation_module.eval()
segmentation_module.cuda()
y = Yandex()
"""## Load test data

Now we load and normalize a single test image.  Here we use the commonplace convention of normalizing the image to a scale for which the RGB values of a large photo dataset would have zero mean and unit standard deviation.  (These numbers come from the imagenet dataset.)  With this normalization, the limiiting ranges of RGB values are within about (-2.2 to +2.7).
"""

# Load and normalize one image as a singleton tensor batch
pil_to_tensor = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(
        mean=[0.485, 0.456, 0.406], # These are RGB mean+std values
        std=[0.229, 0.224, 0.225])  # across a large photo dataset.
])
pil_image = PIL.Image.open('ADE_val_00001519.jpg').convert('RGB')
img_original = numpy.array(pil_image)
img_data = pil_to_tensor(pil_image)
singleton_batch = {'img_data': img_data[None].cuda()}
output_size = img_data.shape[1:]

"""## Run the Model

Finally we just pass the test image to the segmentation model.

The segmentation model is coded as a function that takes a dictionary as input, because it wants to know both the input batch image data as well as the desired output segmentation resolution.  We ask for full resolution output.

Then we use the previously-defined visualize_result function to render the segmentation map.
"""

# Run the segmentation at the highest resolution.
with torch.no_grad():
    scores = segmentation_module(singleton_batch, segSize=output_size)
    #print(scores)
# Get the predicted scores for each pixel
_, pred = torch.max(scores, dim=1)
pred = pred.cpu()[0].numpy()
#visualize_result(img_original, pred)

"""## Showing classes individually

To see which colors are which, here we visualize individual classes, one at a time.
"""

# Top classes in answer
total_price = 0
names = []
predicted_classes = numpy.bincount(pred.flatten()).argsort()[::-1]
for c in predicted_classes[:15]:
    kord = visualize_result(img_original, pred, c)
    
    photo_save(c,kord,pil_image)
    name ,price = y.get_info_yangex(str(c)+".jpg")
    #print('//////---------------\\\\\\\\\\')
    names.append(name)
    
    if price:
        total_price+=price
print('найдены следующие объекты:' + str(names))
print('суммарная цена обнаруженыйх предметов ' + str(total_price))
